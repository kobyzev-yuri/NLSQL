#!/usr/bin/env python3
"""
–°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π Ollama –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ SQL
"""

import sys
import os
import asyncio
import time
import json
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from src.services.query_service import QueryService
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.WARNING, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# –ú–æ–¥–µ–ª–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (–∏—Å–∫–ª—é—á–∞—è llava)
MODELS_TO_TEST = [
    "mistral:7b",
    "sqlcoder:latest", 
    "phi3:latest",
    "llama3:latest",
    "qwen2.5-coder:1.5b",
    "qwen2.5:1.5b",
    "qwen3:8b"
]

# –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Ç–µ—Å—Ç–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã
TEST_QUESTIONS = [
    "–ü–æ–∫–∞–∂–∏ –≤—Å–µ—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π",
    "–°–ø–∏—Å–æ–∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –ø–æ –æ—Ç–¥–µ–ª–∞–º",
    "–ü–æ–∫–∞–∂–∏ –ø–æ—Ä—É—á–µ–Ω–∏—è –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π –º–µ—Å—è—Ü",
    "–ü–ª–∞—Ç–µ–∂–∏ –ø–æ –∫–ª–∏–µ–Ω—Ç–∞–º",
    "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –æ—Ç–¥–µ–ª–∞–º"
]

class ModelTester:
    def __init__(self):
        self.results = {}
        
    async def test_model(self, model_name, question):
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –æ–¥–Ω–æ–º –≤–æ–ø—Ä–æ—Å–µ"""
        try:
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è –º–æ–¥–µ–ª–∏
            os.environ['OLLAMA_MODEL'] = model_name
            
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —Å–µ—Ä–≤–∏—Å –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
            query_service = QueryService()
            
            start_time = time.time()
            sql = await query_service.generate_sql(question, {})
            end_time = time.time()
            
            duration = end_time - start_time
            
            # –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ SQL
            quality_score = self.analyze_sql_quality(sql)
            
            return {
                'sql': sql,
                'duration': duration,
                'quality_score': quality_score,
                'success': quality_score > 0
            }
            
        except Exception as e:
            return {
                'sql': f"ERROR: {str(e)}",
                'duration': 0,
                'quality_score': 0,
                'success': False
            }
    
    def analyze_sql_quality(self, sql):
        """–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ SQL –∑–∞–ø—Ä–æ—Å–∞"""
        if not sql or not sql.strip():
            return 0
            
        sql_lower = sql.lower().strip()
        
        # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
        if not sql_lower.startswith('select'):
            return 0
            
        # –®—Ç—Ä–∞—Ñ—ã –∑–∞ –æ—à–∏–±–∫–∏
        penalty = 0
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ç–∞–±–ª–∏—Ü—ã
        if 'eqorders' in sql_lower or 'eqpayments' in sql_lower:
            penalty += 1
            
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—Ä–æ—Å—Ç—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        if sql_lower == 'select * from equsers;':
            return 3  # –ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            
        # –ë–æ–Ω—É—Å—ã –∑–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç—å
        bonus = 0
        if 'join' in sql_lower:
            bonus += 1
        if 'group by' in sql_lower:
            bonus += 1
        if 'where' in sql_lower:
            bonus += 1
        if 'count(' in sql_lower or 'sum(' in sql_lower:
            bonus += 1
            
        return max(0, 5 - penalty + bonus)
    
    async def run_comparison(self):
        """–ó–∞–ø—É—Å–∫ —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
        print("ü§ñ –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π Ollama")
        print("=" * 80)
        print(f"–ú–æ–¥–µ–ª–∏: {', '.join(MODELS_TO_TEST)}")
        print(f"–í–æ–ø—Ä–æ—Å—ã: {len(TEST_QUESTIONS)}")
        print("=" * 80)
        
        for model in MODELS_TO_TEST:
            print(f"\nüîç –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å: {model}")
            print("-" * 50)
            
            model_results = {
                'model': model,
                'questions': {},
                'summary': {
                    'total_questions': len(TEST_QUESTIONS),
                    'successful_questions': 0,
                    'average_duration': 0,
                    'average_quality': 0,
                    'total_duration': 0
                }
            }
            
            total_duration = 0
            successful_count = 0
            total_quality = 0
            
            for i, question in enumerate(TEST_QUESTIONS, 1):
                print(f"  {i}. {question}")
                
                result = await self.test_model(model, question)
                model_results['questions'][question] = result
                
                total_duration += result['duration']
                if result['success']:
                    successful_count += 1
                total_quality += result['quality_score']
                
                print(f"     ‚úÖ SQL: {result['sql'][:100]}...")
                print(f"     ‚è±Ô∏è  –í—Ä–µ–º—è: {result['duration']:.1f}—Å")
                print(f"     üìä –ö–∞—á–µ—Å—Ç–≤–æ: {result['quality_score']}/5")
            
            # –ü–æ–¥—Å—á–µ—Ç –∏—Ç–æ–≥–æ–≤
            model_results['summary']['successful_questions'] = successful_count
            model_results['summary']['average_duration'] = total_duration / len(TEST_QUESTIONS)
            model_results['summary']['average_quality'] = total_quality / len(TEST_QUESTIONS)
            model_results['summary']['total_duration'] = total_duration
            
            self.results[model] = model_results
            
            print(f"\nüìä –ò—Ç–æ–≥–∏ –¥–ª—è {model}:")
            print(f"   –£—Å–ø–µ—à–Ω—ã—Ö: {successful_count}/{len(TEST_QUESTIONS)} ({successful_count/len(TEST_QUESTIONS)*100:.1f}%)")
            print(f"   –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {model_results['summary']['average_duration']:.1f}—Å")
            print(f"   –°—Ä–µ–¥–Ω–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ: {model_results['summary']['average_quality']:.1f}/5")
            print(f"   –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_duration:.1f}—Å")
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
        self.generate_report()
    
    def generate_report(self):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
        print("\n" + "="*80)
        print("üìä –°–†–ê–í–ù–ò–¢–ï–õ–¨–ù–´–ô –û–¢–ß–ï–¢ –ú–û–î–ï–õ–ï–ô OLLAMA")
        print("="*80)
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É
        sorted_models = sorted(
            self.results.items(), 
            key=lambda x: x[1]['summary']['average_quality'], 
            reverse=True
        )
        
        print("\nüèÜ –†–ï–ô–¢–ò–ù–ì –ü–û –ö–ê–ß–ï–°–¢–í–£:")
        print("-" * 50)
        for i, (model, results) in enumerate(sorted_models, 1):
            summary = results['summary']
            print(f"{i}. {model}")
            print(f"   –ö–∞—á–µ—Å—Ç–≤–æ: {summary['average_quality']:.1f}/5")
            print(f"   –£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {summary['successful_questions']}/{summary['total_questions']} ({summary['successful_questions']/summary['total_questions']*100:.1f}%)")
            print(f"   –í—Ä–µ–º—è: {summary['average_duration']:.1f}—Å")
            print()
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —Å–∫–æ—Ä–æ—Å—Ç–∏
        sorted_by_speed = sorted(
            self.results.items(), 
            key=lambda x: x[1]['summary']['average_duration']
        )
        
        print("‚ö° –†–ï–ô–¢–ò–ù–ì –ü–û –°–ö–û–†–û–°–¢–ò:")
        print("-" * 50)
        for i, (model, results) in enumerate(sorted_by_speed, 1):
            summary = results['summary']
            print(f"{i}. {model}: {summary['average_duration']:.1f}—Å")
        
        # –î–µ—Ç–∞–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞
        print("\nüìã –î–ï–¢–ê–õ–¨–ù–ê–Ø –¢–ê–ë–õ–ò–¶–ê:")
        print("-" * 80)
        print(f"{'–ú–æ–¥–µ–ª—å':<20} {'–ö–∞—á–µ—Å—Ç–≤–æ':<10} {'–£—Å–ø–µ—à–Ω–æ—Å—Ç—å':<12} {'–í—Ä–µ–º—è':<8} {'–û–±—â–µ–µ –≤—Ä–µ–º—è':<12}")
        print("-" * 80)
        
        for model, results in sorted_models:
            summary = results['summary']
            success_rate = f"{summary['successful_questions']}/{summary['total_questions']} ({summary['successful_questions']/summary['total_questions']*100:.1f}%)"
            print(f"{model:<20} {summary['average_quality']:<10.1f} {success_rate:<12} {summary['average_duration']:<8.1f}—Å {summary['total_duration']:<12.1f}—Å")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        with open('ollama_models_comparison.json', 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        
        print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: ollama_models_comparison.json")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        print("\nüéØ –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
        print("-" * 50)
        best_quality = sorted_models[0]
        fastest = sorted_by_speed[0]
        
        print(f"üèÜ –õ—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ: {best_quality[0]} ({best_quality[1]['summary']['average_quality']:.1f}/5)")
        print(f"‚ö° –°–∞–º–∞—è –±—ã—Å—Ç—Ä–∞—è: {fastest[0]} ({fastest[1]['summary']['average_duration']:.1f}—Å)")
        
        # –ù–∞—Ö–æ–¥–∏–º –±–∞–ª–∞–Ω—Å –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Å–∫–æ—Ä–æ—Å—Ç–∏
        balanced_models = []
        for model, results in self.results.items():
            quality = results['summary']['average_quality']
            speed = results['summary']['average_duration']
            # –ë–∞–ª–ª = –∫–∞—á–µ—Å—Ç–≤–æ / –≤—Ä–µ–º—è (—á–µ–º –±–æ–ª—å—à–µ, —Ç–µ–º –ª—É—á—à–µ)
            score = quality / speed if speed > 0 else 0
            balanced_models.append((model, score, quality, speed))
        
        balanced_models.sort(key=lambda x: x[1], reverse=True)
        best_balanced = balanced_models[0]
        
        print(f"‚öñÔ∏è  –õ—É—á—à–∏–π –±–∞–ª–∞–Ω—Å: {best_balanced[0]} (–∫–∞—á–µ—Å—Ç–≤–æ: {best_balanced[2]:.1f}, –≤—Ä–µ–º—è: {best_balanced[3]:.1f}—Å)")

async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    tester = ModelTester()
    await tester.run_comparison()

if __name__ == "__main__":
    asyncio.run(main())




