#!/usr/bin/env python3
"""
–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∏ –ª–µ–∫—Å–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
"""

import asyncio
import asyncpg
import os
import logging
import re
from typing import List, Dict, Any, Tuple
from openai import OpenAI

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SearchMethodComparator:
    """–°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å –º–µ—Ç–æ–¥–æ–≤ –ø–æ–∏—Å–∫–∞"""
    
    def __init__(self):
        self.database_url = "postgresql://postgres:1234@localhost:5432/test_docstructure"
        self.openai_client = OpenAI(
            api_key=os.getenv("PROXYAPI_KEY"),
            base_url="https://api.proxyapi.ru/openai/v1"
        )
    
    async def semantic_search(self, question: str, limit: int = 5) -> List[Dict[str, Any]]:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫"""
        try:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
            response = self.openai_client.embeddings.create(
                model="text-embedding-ada-002",
                input=question
            )
            question_embedding = response.data[0].embedding
            
            # –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ –ë–î
            conn = await asyncpg.connect(self.database_url)
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
            embedding_str = '[' + ','.join(map(str, question_embedding)) + ']'
            
            # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
            query = """
                SELECT content, embedding <-> $1::vector as distance, content_type
                FROM vanna_vectors 
                WHERE embedding IS NOT NULL
                ORDER BY embedding <-> $1::vector
                LIMIT $2
            """
            
            results = await conn.fetch(query, embedding_str, limit)
            await conn.close()
            
            return [dict(row) for row in results]
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
            return []
    
    async def lexical_search(self, question: str, limit: int = 5) -> List[Dict[str, Any]]:
        """–õ–µ–∫—Å–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ (–ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º)"""
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞
            keywords = self._extract_keywords(question)
            if not keywords:
                return []
            
            # –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ –ë–î
            conn = await asyncpg.connect(self.database_url)
            
            # –°—Ç—Ä–æ–∏–º –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            search_conditions = []
            params = []
            
            for i, keyword in enumerate(keywords):
                search_conditions.append(f"content ILIKE ${i + 1}")
                params.append(f"%{keyword}%")
            
            if not search_conditions:
                await conn.close()
                return []
            
            # –õ–µ–∫—Å–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
            query = f"""
                SELECT content, 
                       CASE 
                           WHEN content ILIKE ANY(${len(params) + 1}) THEN 1.0
                           ELSE 0.5
                       END as relevance,
                       content_type
                FROM vanna_vectors 
                WHERE {' OR '.join(search_conditions)}
                ORDER BY relevance DESC, content
                LIMIT ${len(params) + 2}
            """
            
            # –î–æ–±–∞–≤–ª—è–µ–º –º–∞—Å—Å–∏–≤ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            params.append([f"%{kw}%" for kw in keywords])
            params.append(limit)
            
            results = await conn.fetch(query, *params)
            await conn.close()
            
            return [dict(row) for row in results]
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ª–µ–∫—Å–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
            return []
    
    def _extract_keywords(self, question: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞"""
        # –£–±–∏—Ä–∞–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏ –∏–∑–≤–ª–µ–∫–∞–µ–º –∑–Ω–∞—á–∏–º—ã–µ —Å–ª–æ–≤–∞
        stop_words = {
            '–∏', '–≤', '–Ω–∞', '—Å', '–ø–æ', '–¥–ª—è', '–æ—Ç', '–¥–æ', '–∑–∞', '–∏–∑', '–∫', '–æ', '–æ–±', '–ø—Ä–∏', '–ø—Ä–æ', '—Å–æ', '—É',
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'
        }
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞ –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º
        words = re.findall(r'\b\w+\b', question.lower())
        keywords = [word for word in words if len(word) > 2 and word not in stop_words]
        
        return keywords
    
    async def hybrid_search(self, question: str, limit: int = 5, semantic_weight: float = 0.7) -> List[Dict[str, Any]]:
        """–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ (–∫–æ–º–±–∏–Ω–∞—Ü–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∏ –ª–µ–∫—Å–∏—á–µ—Å–∫–æ–≥–æ)"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±–æ–∏—Ö –º–µ—Ç–æ–¥–æ–≤
            semantic_results = await self.semantic_search(question, limit * 2)
            lexical_results = await self.lexical_search(question, limit * 2)
            
            # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            combined_results = {}
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            for result in semantic_results:
                content = result['content']
                if content not in combined_results:
                    combined_results[content] = {
                        'content': content,
                        'content_type': result['content_type'],
                        'semantic_score': 1.0 - result['distance'],  # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –≤ score
                        'lexical_score': 0.0,
                        'hybrid_score': 0.0
                    }
                else:
                    combined_results[content]['semantic_score'] = 1.0 - result['distance']
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ª–µ–∫—Å–∏—á–µ—Å–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            for result in lexical_results:
                content = result['content']
                if content not in combined_results:
                    combined_results[content] = {
                        'content': content,
                        'content_type': result['content_type'],
                        'semantic_score': 0.0,
                        'lexical_score': result['relevance'],
                        'hybrid_score': 0.0
                    }
                else:
                    combined_results[content]['lexical_score'] = result['relevance']
            
            # –í—ã—á–∏—Å–ª—è–µ–º –≥–∏–±—Ä–∏–¥–Ω—ã–π score
            for content, result in combined_results.items():
                semantic_score = result['semantic_score']
                lexical_score = result['lexical_score']
                
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º scores –∫ 0-1
                semantic_score = min(1.0, max(0.0, semantic_score))
                lexical_score = min(1.0, max(0.0, lexical_score))
                
                # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è
                hybrid_score = (semantic_weight * semantic_score + 
                              (1 - semantic_weight) * lexical_score)
                
                result['semantic_score'] = semantic_score
                result['lexical_score'] = lexical_score
                result['hybrid_score'] = hybrid_score
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≥–∏–±—Ä–∏–¥–Ω–æ–º—É score
            final_results = list(combined_results.values())
            final_results.sort(key=lambda x: x['hybrid_score'], reverse=True)
            
            return final_results[:limit]
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
            return []
    
    async def compare_methods(self, question: str) -> Dict[str, Any]:
        """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤ –ø–æ–∏—Å–∫–∞"""
        print(f"\nüîç –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ –ø–æ–∏—Å–∫–∞ –¥–ª—è: '{question}'")
        print("=" * 60)
        
        # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
        print("\nüìä –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫:")
        semantic_results = await self.semantic_search(question, 3)
        for i, result in enumerate(semantic_results, 1):
            distance = result['distance']
            content_type = result['content_type']
            content_preview = result['content'][:100] + "..." if len(result['content']) > 100 else result['content']
            print(f"  {i}. [{content_type}] (—Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ: {distance:.4f})")
            print(f"     {content_preview}")
        
        # –õ–µ–∫—Å–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
        print("\nüìä –õ–µ–∫—Å–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫:")
        lexical_results = await self.lexical_search(question, 3)
        for i, result in enumerate(lexical_results, 1):
            relevance = result['relevance']
            content_type = result['content_type']
            content_preview = result['content'][:100] + "..." if len(result['content']) > 100 else result['content']
            print(f"  {i}. [{content_type}] (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {relevance:.4f})")
            print(f"     {content_preview}")
        
        # –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫
        print("\nüìä –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫:")
        hybrid_results = await self.hybrid_search(question, 3)
        for i, result in enumerate(hybrid_results, 1):
            hybrid_score = result['hybrid_score']
            semantic_score = result['semantic_score']
            lexical_score = result['lexical_score']
            content_type = result['content_type']
            content_preview = result['content'][:100] + "..." if len(result['content']) > 100 else result['content']
            print(f"  {i}. [{content_type}] (–≥–∏–±—Ä–∏–¥–Ω—ã–π: {hybrid_score:.4f}, —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π: {semantic_score:.4f}, –ª–µ–∫—Å–∏—á–µ—Å–∫–∏–π: {lexical_score:.4f})")
            print(f"     {content_preview}")
        
        return {
            'semantic': semantic_results,
            'lexical': lexical_results,
            'hybrid': hybrid_results
        }

async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
    comparator = SearchMethodComparator()
    
    print("üöÄ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ –ø–æ–∏—Å–∫–∞...")
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã
    test_questions = [
        "–ü–ª–∞—Ç–µ–∂–∏ –∑–∞ –º–µ—Å—è—Ü –ø–æ –∫–ª–∏–µ–Ω—Ç–∞–º",
        "–ü–æ–∫–∞–∂–∏ –≤—Å–µ –ø–ª–∞—Ç–µ–∂–∏",
        "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø–ª–∞—Ç–µ–∂–∞–º",
        "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ —Å–∏—Å—Ç–µ–º—ã",
        "–ü–æ–∫–∞–∂–∏ —Ç–∞–±–ª–∏—Ü—ã —Å –ø–ª–∞—Ç–µ–∂–∞–º–∏"
    ]
    
    for question in test_questions:
        await comparator.compare_methods(question)
        print("\n" + "=" * 80)

if __name__ == "__main__":
    asyncio.run(main())
